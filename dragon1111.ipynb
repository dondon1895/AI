{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpC7h+W+2eavGMV92RP8GZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dondon1895/AI/blob/master/dragon1111.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I7D3qPbqsaJ"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "\n",
        "def printGPU():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "  \n",
        "printGPU()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4XUM90eqW6i"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgzF7PsqoVL"
      },
      "source": [
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtBITxOWkJs5"
      },
      "source": [
        "!python3 --version\n",
        "!pip3 --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1x37EWzklF9"
      },
      "source": [
        "!sudo apt update\n",
        "!sudo apt install python3-dev python3-pip python3-venv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aXhbZfEpSn_"
      },
      "source": [
        "!python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30pEhhvncAx"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52S4tZ3DjCAX"
      },
      "source": [
        "!wget https://yarnpkg.com/latest.tar.gz\n",
        "!tar zvxf latest.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgYRu4vSjW1u"
      },
      "source": [
        "!wget -qO- https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --import\n",
        "!wget https://yarnpkg.com/latest.tar.gz.asc\n",
        "!gpg --verify latest.tar.gz.asc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3583qYTMjk7h"
      },
      "source": [
        "!npm install --global yarn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLTAUJ9Kjsk7"
      },
      "source": [
        "!yarn --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzdWIOSYh20P"
      },
      "source": [
        "!yarn && yarn watch\n",
        "!yarn\n",
        "!yarn train shakespeare \\\n",
        "    --lstmLayerSize 128,128 \\\n",
        "    --epochs 120 \\\n",
        "    --savePath ./my-shakespeare-model\n",
        "!yarn gen shakespeare ./my-shakespeare-model/model.json \\\n",
        "    --genLength 250 \\\n",
        "    --temperature 0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFnkMH_CAf4-"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "text_as_int1 = tf.keras.utils.get_file('alllog.txt','https://raw.githubusercontent.com/dondon1895/work1104/main/alllog.TXT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9BpLcy5CSd9"
      },
      "source": [
        "text = open(text_as_int1, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJkCSfJrCMuv"
      },
      "source": [
        "print(text[9505:9702])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fFE7DQTCc6K"
      },
      "source": [
        "n = len(text)\n",
        "w = len(set(text))\n",
        "print(f\"金瓶梅小說共有 {n} 中文字\")\n",
        "print(f\"包含了 {w} 個獨一無二的字\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC7s66ODOhcZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bajWRJsiDNzI"
      },
      "source": [
        "\n",
        "# 初始化一個以字為單位的 Tokenizer\n",
        "num_words=4499\n",
        "tokenizer = tf.keras\\\n",
        "    .preprocessing\\\n",
        "    .text\\\n",
        "    .Tokenizer(\n",
        "        num_words=num_words,\n",
        "        char_level=True,\n",
        "        filters=''\n",
        ")\n",
        "    \n",
        "tokenizer.fit_on_texts(text)\n",
        "text_as_int = tokenizer\\\n",
        "        .texts_to_sequences([text])[0]\n",
        "\n",
        "# 隨機選取一個片段文本方便之後做說明\n",
        "s_idx = 21004\n",
        "e_idx = 21020\n",
        "partial_indices = \\\n",
        "    text_as_int[s_idx:e_idx]\n",
        "partial_texts = [\n",
        "    tokenizer.index_word[idx] \\\n",
        "    for idx in partial_indices\n",
        "]\n",
        "\n",
        "# 渲染結果，可忽略\n",
        "print(\"原本的中文字序列：\")\n",
        "print()\n",
        "print(partial_texts)\n",
        "print()\n",
        "print(\"-\" * 20)\n",
        "print()\n",
        "print(\"轉換後的索引序列：\")\n",
        "print()\n",
        "print(partial_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWvGAi3HPVoS"
      },
      "source": [
        "text_as_int[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHBr_S2nQxax"
      },
      "source": [
        "_type = type(text_as_int)\n",
        "n = len(text_as_int)\n",
        "print(f\"text_as_int 是一個 {_type}\\n\")\n",
        "print(f\"小說的序列長度： {n}\\n\")\n",
        "print(\"前 5 索引：\", text_as_int[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1XyxHHwQz-k"
      },
      "source": [
        "print(\"實際丟給模型的數字序列：\")\n",
        "print(partial_indices[:-1])\n",
        "print()\n",
        "print(\"方便我們理解的文本序列：\")\n",
        "print(partial_texts[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q4X40sWQ3VX"
      },
      "source": [
        "# 方便說明，實際上我們會用更大的值來\n",
        "# 讓模型從更長的序列預測下個中文字\n",
        "SEQ_LENGTH = 10  # 數字序列長度\n",
        "BATCH_SIZE = 128 # 幾筆成對輸入/輸出\n",
        "\n",
        "# text_as_int 是一個 python list\n",
        "# 我們利用 from_tensor_slices 將其\n",
        "# 轉變成 TensorFlow 最愛的 Tensor <3\n",
        "characters = tf\\\n",
        "    .data\\\n",
        "    .Dataset\\\n",
        "    .from_tensor_slices(\n",
        "        text_as_int)\n",
        "\n",
        "# 將被以數字序列表示的天龍八部文本\n",
        "# 拆成多個長度為 SEQ_LENGTH (10) 的序列\n",
        "# 並將最後長度不滿 SEQ_LENGTH 的序列捨去\n",
        "sequences = characters\\\n",
        "    .batch(SEQ_LENGTH + 1, \n",
        "           drop_remainder=True)\n",
        "\n",
        "# 天龍八部全文所包含的成對輸入/輸出的數量\n",
        "steps_per_epoch = \\\n",
        "    len(text_as_int) // SEQ_LENGTH\n",
        "\n",
        "# 這個函式專門負責把一個序列\n",
        "# 拆成兩個序列，分別代表輸入與輸出\n",
        "# （下段有 vis 解釋這在做什麼）\n",
        "def build_seq_pairs(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# 將每個從文本擷取出來的序列套用上面\n",
        "# 定義的函式，拆成兩個數字序列\n",
        "# 作為輸入／輸出序列\n",
        "# 再將得到的所有數據隨機打亂順序\n",
        "# 最後再一次拿出 BATCH_SIZE（128）筆數據\n",
        "# 作為模型一次訓練步驟的所使用的資料\n",
        "ds = sequences\\\n",
        "    .map(build_seq_pairs)\\\n",
        "    .shuffle(steps_per_epoch)\\\n",
        "    .batch(BATCH_SIZE, \n",
        "           drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0WtL1F1RG1A"
      },
      "source": [
        "# print 是用來幫你理解 tf.data.Dataset\n",
        "# 的內容，實際上存取資料集非常簡單\n",
        "# 現在先關注下面的 print 結果\n",
        "for b_inp, b_tar in ds.take(1):\n",
        "    print(\"起始句子的 batch：\")\n",
        "    print(b_inp, \"\\n\")\n",
        "    print(\"目標句子的 batch：\")\n",
        "    print(b_tar, \"\\n\")\n",
        "    print(\"-\" * 20, \"\\n\")\n",
        "    \n",
        "    print(\"第一個起始句子的索引序列：\")\n",
        "    first_i = b_inp.numpy()[0]\n",
        "    print(first_i, \"\\n\")\n",
        "    print(\"第一個目標句子的索引序列：\")\n",
        "    first_t = b_tar.numpy()[0]\n",
        "    print(first_t, \"\\n\")\n",
        "    print(\"-\" * 20, \"\\n\")\n",
        "    \n",
        "    d = tokenizer.index_word\n",
        "    print(\"第一個起始句子的文本序列：\")\n",
        "    print([d[i] for i in first_i])\n",
        "    print()\n",
        "    print(\"第一個目標句子的文本序列：\")\n",
        "    print([d[i] for i in first_t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAFJEzImRP6M"
      },
      "source": [
        "for b_inp, b_tar in ds.take(1):\n",
        "    # 蒙多想去哪就去哪\n",
        "    # 想怎麼存取 b_iup, b_tar 都可以\n",
        "    print(\"b_inp 是個 Tensor：\\n\")\n",
        "    print(b_inp)\n",
        "    print(\"\\nb_tar 也是個 Tensor，\")\n",
        "    print(\"只是每個數字序列都是\"\n",
        "          \"對應的輸入序列往左位\"\n",
        "          \"移一格的結果\\n\")\n",
        "    print(b_tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ccWZn8PRUk3"
      },
      "source": [
        "# 超參數\n",
        "EMBEDDING_DIM = 512\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# 使用 keras 建立一個非常簡單的 LSTM 模型\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# 詞嵌入層\n",
        "# 將每個索引數字對應到一個高維空間的向量\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=num_words, \n",
        "        output_dim=EMBEDDING_DIM,\n",
        "        batch_input_shape=[\n",
        "            BATCH_SIZE, None]\n",
        "))\n",
        "\n",
        "# LSTM 層\n",
        "# 負責將序列數據依序讀入並做處理\n",
        "model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "    units=RNN_UNITS, \n",
        "    return_sequences=True, \n",
        "    stateful=True, \n",
        "    recurrent_initializer='glorot_uniform'\n",
        "))\n",
        "\n",
        "# 全連接層\n",
        "# 負責 model 每個中文字出現的可能性\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        num_words))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xrHVmE8RYb8"
      },
      "source": [
        "# 超參數，決定模型一次要更新的步伐有多大\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# 定義模型預測結果跟正確解答之間的差異\n",
        "# 因為全連接層沒使用 activation func\n",
        "# from_logits= True \n",
        "def loss(y_true, y_pred):\n",
        "    return tf.keras.losses\\\n",
        "    .sparse_categorical_crossentropy(\n",
        "        y_true, y_pred, from_logits=True)\n",
        "\n",
        "# 編譯模型，使用 Adam Optimizer 來最小化\n",
        "# 剛剛定義的損失函數\n",
        "model.compile(\n",
        "    optimizer=tf.keras\\\n",
        "        .optimizers.Adam(\n",
        "        learning_rate=LEARNING_RATE), \n",
        "    loss=loss\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPJo6xqxRdA5"
      },
      "source": [
        "EPOCHS = 10 # 決定看幾篇金瓶梅文本\n",
        "history = model.fit(\n",
        "    ds, # 前面使用 tf.data 建構的資料集\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMHB5eEuBjpi"
      },
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks\\\n",
        "        .TensorBoard(\"logs\"),\n",
        "    # 你可以加入其他 callbacks 如\n",
        "    # ModelCheckpoint,\n",
        "    # EarlyStopping\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    ds,\n",
        "    epochs=EPOCHS, \n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyBt-o7L9sx5"
      },
      "source": [
        "pip install -U tensorboard-plugin-profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKdMIFRUrMWu"
      },
      "source": [
        "!tensorboard dev upload \\\n",
        "  --logdir logs/fit \\\n",
        "  --name \"(optional) My latest experiment\" \\\n",
        "  --description \"(optional) Simple comparison of several hyperparameters\" \\\n",
        "  --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfiRnAGUBrRd"
      },
      "source": [
        "\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDXIliqqEBqI"
      },
      "source": [
        "# 方便說明，實際上我們會用更大的值來\n",
        "# 讓模型從更長的序列預測下個中文字\n",
        "SEQ_LENGTH = 10  # 數字序列長度\n",
        "BATCH_SIZE = 128 # 幾筆成對輸入/輸出\n",
        "\n",
        "# text_as_int 是一個 python list\n",
        "# 我們利用 from_tensor_slices 將其\n",
        "# 轉變成 TensorFlow 最愛的 Tensor <3\n",
        "characters = tf\\\n",
        "    .data\\\n",
        "    .Dataset\\\n",
        "    .from_tensor_slices(\n",
        "        text_as_int)\n",
        "\n",
        "# 將被以數字序列表示的天龍八部文本\n",
        "# 拆成多個長度為 SEQ_LENGTH (10) 的序列\n",
        "# 並將最後長度不滿 SEQ_LENGTH 的序列捨去\n",
        "sequences = characters\\\n",
        "    .batch(SEQ_LENGTH + 1, \n",
        "           drop_remainder=True)\n",
        "\n",
        "# 天龍八部全文所包含的成對輸入/輸出的數量\n",
        "steps_per_epoch = \\\n",
        "    len(text_as_int) // SEQ_LENGTH\n",
        "\n",
        "# 這個函式專門負責把一個序列\n",
        "# 拆成兩個序列，分別代表輸入與輸出\n",
        "# （下段有 vis 解釋這在做什麼）\n",
        "def build_seq_pairs(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# 將每個從文本擷取出來的序列套用上面\n",
        "# 定義的函式，拆成兩個數字序列\n",
        "# 作為輸入／輸出序列\n",
        "# 再將得到的所有數據隨機打亂順序\n",
        "# 最後再一次拿出 BATCH_SIZE（128）筆數據\n",
        "# 作為模型一次訓練步驟的所使用的資料\n",
        "ds = sequences\\\n",
        "    .map(build_seq_pairs)\\\n",
        "    .shuffle(steps_per_epoch)\\\n",
        "    .batch(BATCH_SIZE, \n",
        "           drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNQM_y1zZEVS"
      },
      "source": [
        "# 跟訓練時一樣的超參數，\n",
        "# 只差在 BATCH_SIZE 為 1\n",
        "EMBEDDING_DIM = 512\n",
        "RNN_UNITS = 1024\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# 專門用來做生成的模型\n",
        "infer_model = tf.keras.Sequential()\n",
        "\n",
        "# 詞嵌入層\n",
        "infer_model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=num_words, \n",
        "        output_dim=EMBEDDING_DIM,\n",
        "        batch_input_shape=[\n",
        "            BATCH_SIZE, None]\n",
        "))\n",
        "\n",
        "# LSTM 層\n",
        "infer_model.add(\n",
        "    tf.keras.layers.LSTM(\n",
        "    units=RNN_UNITS, \n",
        "    return_sequences=True, \n",
        "    stateful=True\n",
        "))\n",
        "\n",
        "# 全連接層\n",
        "infer_model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        num_words))\n",
        "        \n",
        "infer_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENk3hRUgCWq1"
      },
      "source": [
        "# 超參數，決定模型一次要更新的步伐有多大\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# 定義模型預測結果跟正確解答之間的差異\n",
        "# 因為全連接層沒使用 activation func\n",
        "# from_logits= True \n",
        "def loss(y_true, y_pred):\n",
        "    return tf.keras.losses\\\n",
        "    .sparse_categorical_crossentropy(\n",
        "        y_true, y_pred, from_logits=True)\n",
        "\n",
        "# 編譯模型，使用 Adam Optimizer 來最小化\n",
        "# 剛剛定義的損失函數\n",
        "infer_model.compile(\n",
        "    optimizer=tf.keras\\\n",
        "        .optimizers.Adam(\n",
        "        learning_rate=LEARNING_RATE), \n",
        "    loss=loss\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcicVjSedCgu"
      },
      "source": [
        "infer_model.load_weights(ckpt_path)\n",
        "infer_model.build(\n",
        "    tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXYjwWYMf_A7"
      },
      "source": [
        "# 代表「喬」的索引\n",
        "seed_indices = [234] \n",
        "\n",
        "# 增加 batch 維度丟入模型取得預測結果後\n",
        "# 再度降維，拿掉 batch 維度\n",
        "input = tf.expand_dims(\n",
        "    seed_indices, axis=0)\n",
        "predictions = infer_model(input)\n",
        "predictions = tf.squeeze(\n",
        "    predictions, 0)\n",
        "\n",
        "# 利用生成溫度影響抽樣結果\n",
        " predictions \\= temperature\n",
        "\n",
        "# 從 4330 個分類值中做抽樣\n",
        "# 取得這個時間點模型生成的中文字\n",
        "sampled_indices = tf.random\\\n",
        "    .categorical(\n",
        "        predictions, num_samples=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ3k4URe-kgL"
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I96U6wVE8dr2",
        "outputId": "6795af40-1913-4ae5-cb5e-1ce95bf3f9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "tensorflowjs_converter \\\n",
        "    --input_format=keras \\\n",
        "    model.h5 \\\n",
        "    tfjs_model_folder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-f6023725e9e4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorflowjs_converter     --input_format=keras     model.h5     tfjs_model_folder\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e31kAKzz9dAl",
        "outputId": "9e6b259f-7799-4836-aa46-970bc783390c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "model = tf.loadLayersModel(\"url\");\n",
        "const output = model.predict(input);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-55-b2c14ea4ed30>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    const output = model.predict(input);\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SuZHUtwHZzB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}